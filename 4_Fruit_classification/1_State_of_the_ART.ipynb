{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Bases\n",
    "import keras as k\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "\n",
    "## data\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "## building\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, Dropout, MaxPooling2D, LayerNormalization, GlobalAveragePooling2D\n",
    "\n",
    "## plotting\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## callbacks\n",
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "from keras.applications import ResNet152\n",
    "from keras import Input, Model\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import  classification_report, roc_curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Configuration for GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPU, 2 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "memory_limit=1024\n",
    "if gpus:\n",
    "  # Create 2 virtual GPUs with 1GB memory each\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(\n",
    "        gpus[0],\n",
    "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit),\n",
    "         #tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit),\n",
    "         tf.config.experimental.VirtualDeviceConfiguration(memory_limit=memory_limit)])\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPU,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_CPU:0', device_type='XLA_CPU'),\n",
       " PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU'),\n",
       " PhysicalDevice(name='/physical_device:XLA_GPU:0', device_type='XLA_GPU')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 13463972535812607633\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6731419378047132944\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 1073741824\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7074386158972131739\n",
      "physical_device_desc: \"device: 0, name: GeForce GTX 1650, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 17821101728704306533\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset has the following directory ***structure***:\n",
    "\n",
    "<pre>\n",
    "<b>Fruit Images Dataset</b>\n",
    "|__ <b>train</b>\n",
    "    |______ <b>Apple Braeburn</b>: [0_100.jpg, ..]\n",
    "    |______ <b>Apple Crimson Snow</b>: [0_100.jpg, ..] \n",
    "    ..\n",
    "    |______ <b>Apple Watermelon</b>: [0_100.jpg, ..]\n",
    "|__ <b>test</b>\n",
    "    |______ <b>Apple Braeburn</b>: [0_100.jpg, ..]\n",
    "    |______ <b>Apple Crimson Snow</b>: [0_100.jpg, ..] \n",
    "    ..\n",
    "    |______ <b>Apple Watermelon</b>: [0_100.jpg, ..]\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How to prepare data***\n",
    "\n",
    "Format the images into appropriately pre-processed floating point tensors before feeding to the network:\n",
    "\n",
    "1. Read images from the disk.\n",
    "2. Decode contents of these images and convert it into proper grid format as per their RGB content.\n",
    "3. Convert them into floating point tensors.\n",
    "4. Rescale the tensors from values between 0 and 255 to values between 0 and 1, as neural networks prefer to deal with small input values.\n",
    "\n",
    "Fortunately, all these tasks can be done with the `ImageDataGenerator` class provided by `tf.keras`, which can:\n",
    "* Read images and preprocess them into proper tensors. \n",
    "* Set up generators that convert these images into batches of tensors â€” helpful when training the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator\n",
    "train_generator = ImageDataGenerator(rescale=1/255, validation_split=0.2,\n",
    "                                     horizontal_flip=True,\n",
    "                                      rotation_range=45)\n",
    "test_generator = ImageDataGenerator(rescale=1/255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable for pre-processing and training\n",
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "IMG_HEIGHT = 100\n",
    "IMG_WIDTH = 100\n",
    "\n",
    "num_classes = 131\n",
    "\n",
    "where_train = '/home/maihai/GitHub/Fruit-Images-Dataset/train'\n",
    "where_test  = '/home/maihai/GitHub/Fruit-Images-Dataset/test'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After defining the generators, the `flow_from_directory` method will:\n",
    "* Load images from the disk,\n",
    "* Applies rescaling,\n",
    "* Applies resizes images into the required dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 54190 images belonging to 131 classes.\n",
      "Found 13502 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data_gen = train_generator.flow_from_directory(directory=where_train, \n",
    "                                         target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                         class_mode='categorical',\n",
    "                                         shuffle=False,\n",
    "                                         batch_size=batch_size,\n",
    "                                         subset='training')\n",
    "\n",
    "val_data_gen = train_generator.flow_from_directory(directory=where_train,\n",
    "                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=False,\n",
    "                                        batch_size=batch_size,\n",
    "                                        subset='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22688 images belonging to 131 classes.\n"
     ]
    }
   ],
   "source": [
    "test_data_gen = test_generator.flow_from_directory(directory=where_test,\n",
    "                                        target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "                                        class_mode='categorical',\n",
    "                                        shuffle=True,\n",
    "                                        batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The State-of-the-art: ResNet 152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***ResNet 152***, published by Google in 2015. ResNet 152 as its name includes a very deep networks of 152 layers, addressing the problem of information vanishing by Resudual architecture, which skip connection while computing.\n",
    "\n",
    "ResNet was the winner of all stars in 2015, then embeded in Keras with pre-trained weighted of ***'imagenet'***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The typical transfer learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Workflow 1***\n",
    "\n",
    "In this project, we will use transfer learning technique to apply ResNet 152 architecture on our Fruit dataset.\n",
    "\n",
    "1. ***Instantiate*** the ResNet then load pre-trained weights.\n",
    "2. ***Freeze*** all layers by setting ResNet152.trainable = False.\n",
    "3. ***Create*** our custom layers on top of the Resnet's output.\n",
    "4. ***Train*** only our custom layers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Workflow 2***  \n",
    "\n",
    "Workflow 1 has a backdraw in my computer: out of memory. Though we marked ResNet's core layers to be skiped while training, but all of them still be loaded to memory, the number is around 60 millions. The number that big is unable for my computer to work with. So, let's try an alternative approach, much cheaper and more lightweight.  \n",
    "1. ***Instantiate*** the ResNet and load pre=trained weights.\n",
    "2. ***Run*** our Fruit dataset through it, then receive the output. This is called ***feature extraction***.\n",
    "3. ***Use*** that output as input for a new, smaller model.  \n",
    "\n",
    "By this workflow, the need for memory would highly decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Fine tuning***  \n",
    "\n",
    "Either your choice is Workflow 1 or 2, afterall, we have a basical transfer-learning models, with most of weights belong to the core ResNet and a minority of our custom layers. Then, we need to adjust those ResNet weights to the Fruit dataset. The steps are follow:\n",
    "\n",
    "1. ***Unfreeze*** ResNet's core layers by setting ResNet152.trainable = True.\n",
    "2. ***Train*** the entire model, both top layers and the core.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The workflow 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100, 100, 3)       0         \n",
      "_________________________________________________________________\n",
      "resnet152 (Model)            (None, 4, 4, 2048)        58370944  \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 131)               268419    \n",
      "=================================================================\n",
      "Total params: 58,639,363\n",
      "Trainable params: 268,419\n",
      "Non-trainable params: 58,370,944\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 1 Instantiate\n",
    "resnet = ResNet152(weights='imagenet',\n",
    "                   input_shape=(IMG_HEIGHT, IMG_WIDTH, 3),\n",
    "                   include_top=False) # to adopt our fruit classifier\n",
    "# 2 Freeze\n",
    "resnet.trainable = False\n",
    "\n",
    "# 3 Create new model on top\n",
    "inputs  = Input(shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
    "x       = resnet(inputs)\n",
    "x = k.layers.GlobalAveragePooling2D()(x) # Convert features of shape `resnet.output_shape[1:]` to vectors\n",
    "outputs = k.layers.Dense(num_classes)(x)\n",
    "\n",
    "resnet_copycat = Model(inputs, outputs)\n",
    "resnet_copycat.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_copycat.compile(optimizer='adam',\n",
    "                       loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "                       metrics=['categorical_accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Checkpoint + TensorBoard\n",
    "checkpoint_path = 'ResNet152'\n",
    "checkpoint_callback = k.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                         save_weights_only=True,\n",
    "                                                         monitor='val_acc',\n",
    "                                                         mode='max',\n",
    "                                                         save_best_only=True)\n",
    "\n",
    "NAME = 'Resnet_copycat_flow_1_{}'.format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "528/528 [==============================] - 440s 833ms/step - loss: 8.8150 - categorical_accuracy: 0.0036 - val_loss: 16.1181 - val_categorical_accuracy: 0.0019\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/maihai/anaconda3/envs/DLgpu/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "528/528 [==============================] - 427s 808ms/step - loss: 9.1987 - categorical_accuracy: 0.0077 - val_loss: 2.1059 - val_categorical_accuracy: 0.0089\n",
      "Epoch 3/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 8.8910 - categorical_accuracy: 0.0065 - val_loss: 16.1181 - val_categorical_accuracy: 0.0084\n",
      "Epoch 4/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 9.2183 - categorical_accuracy: 0.0085 - val_loss: 0.3778 - val_categorical_accuracy: 0.0087\n",
      "Epoch 5/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 9.0236 - categorical_accuracy: 0.0079 - val_loss: 1.1921e-07 - val_categorical_accuracy: 0.0044\n",
      "Epoch 6/15\n",
      "528/528 [==============================] - 428s 811ms/step - loss: 9.0230 - categorical_accuracy: 0.0127 - val_loss: 16.1181 - val_categorical_accuracy: 0.0087\n",
      "Epoch 7/15\n",
      "528/528 [==============================] - 429s 812ms/step - loss: 9.2822 - categorical_accuracy: 0.0081 - val_loss: 2.1433 - val_categorical_accuracy: 0.0087\n",
      "Epoch 8/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 9.1720 - categorical_accuracy: 0.0109 - val_loss: 0.4105 - val_categorical_accuracy: 0.0044\n",
      "Epoch 9/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 8.9267 - categorical_accuracy: 0.0084 - val_loss: 12.5390 - val_categorical_accuracy: 0.0087\n",
      "Epoch 10/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 9.1975 - categorical_accuracy: 0.0082 - val_loss: 16.1181 - val_categorical_accuracy: 0.0087\n",
      "Epoch 11/15\n",
      "528/528 [==============================] - 428s 811ms/step - loss: 9.6957 - categorical_accuracy: 0.0088 - val_loss: 1.1921e-07 - val_categorical_accuracy: 0.0044\n",
      "Epoch 12/15\n",
      "528/528 [==============================] - 428s 811ms/step - loss: 8.6576 - categorical_accuracy: 0.0077 - val_loss: 11.5849 - val_categorical_accuracy: 0.0087\n",
      "Epoch 13/15\n",
      "528/528 [==============================] - 428s 810ms/step - loss: 8.8943 - categorical_accuracy: 0.0061 - val_loss: 10.4516 - val_categorical_accuracy: 0.0087\n",
      "Epoch 14/15\n",
      "528/528 [==============================] - 429s 812ms/step - loss: 9.3294 - categorical_accuracy: 0.0076 - val_loss: 16.1181 - val_categorical_accuracy: 0.0074\n",
      "Epoch 15/15\n",
      "528/528 [==============================] - 428s 811ms/step - loss: 8.8530 - categorical_accuracy: 0.0127 - val_loss: 8.0773 - val_categorical_accuracy: 0.0057\n",
      "CPU times: user 1h 39min 31s, sys: 21min 29s, total: 2h 1min\n",
      "Wall time: 1h 47min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 4 Training our top layers\n",
    "history = resnet_copycat.fit(train_data_gen,\n",
    "                             steps_per_epoch=67692 // batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=val_data_gen,\n",
    "                             validation_steps=22688 // batch_size,\n",
    "                             callbacks=[tensorboard, checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_copycat.save_weights('resnet_copycat_weights', overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear VRAM\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tuning of the entire model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's unfreeze the base model and train the entire model end-to-end with a low learning rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "resnet.trainable = True\n",
    "\n",
    "resnet_copycat.compile(optimizer='adam',\n",
    "                       loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "                       metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "#### Please, don't run this block. \n",
    "## GPU has not enough VRAM to train it.\n",
    "## CPU got 38 mins for each epoch.\n",
    "## This block is so expensive, don't run it.\n",
    "\n",
    "# Setting up TensorBoard\n",
    "NAME = 'Handmade_model_resnet_copycat_fineTune{}'.format(int(time.time()))\n",
    "tensorboard = TensorBoard(log_dir='logs/{}'.format(NAME))\n",
    "\n",
    "# Training\n",
    "with tf.device('/cpu:0'):\n",
    "    history = resnet_copycat.fit(train_data_gen,\n",
    "                             steps_per_epoch=67692 // batch_size,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=val_data_gen,\n",
    "                             validation_steps=22688 // batch_size,\n",
    "                             callbacks=[tensorboard]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_copycat.load_weights('resnet_copycat_weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178/177 [==============================] - 120s 675ms/step\n",
      "CPU times: user 1min 43s, sys: 24.2 s, total: 2min 8s\n",
      "Wall time: 2min\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "y_pred = resnet_copycat.predict(test_data_gen,\n",
    "                                    steps=22688/batch_size,\n",
    "                                    verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.argmax(y_pred[:22688], axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       164\n",
      "           1       0.00      0.00      0.00       148\n",
      "           2       0.00      0.00      0.00       160\n",
      "           3       0.00      0.00      0.00       164\n",
      "           4       0.00      0.00      0.00       161\n",
      "           5       0.00      0.00      0.00       164\n",
      "           6       0.00      0.00      0.00       152\n",
      "           7       0.00      0.00      0.00       164\n",
      "           8       0.00      0.00      0.00       164\n",
      "           9       0.00      0.00      0.00       144\n",
      "          10       0.00      0.00      0.00       166\n",
      "          11       0.00      0.00      0.00       164\n",
      "          12       0.00      0.00      0.00       219\n",
      "          13       0.00      0.00      0.00       164\n",
      "          14       0.00      0.00      0.00       143\n",
      "          15       0.00      0.00      0.00       166\n",
      "          16       0.00      0.00      0.00       166\n",
      "          17       0.00      0.00      0.00       152\n",
      "          18       0.00      0.00      0.00       166\n",
      "          19       0.00      0.00      0.00       150\n",
      "          20       0.00      0.00      0.00       154\n",
      "          21       0.00      0.00      0.00       166\n",
      "          22       0.00      0.00      0.00       164\n",
      "          23       0.00      0.00      0.00       164\n",
      "          24       0.00      0.00      0.00       166\n",
      "          25       0.00      0.00      0.00       234\n",
      "          26       0.00      0.00      0.00       164\n",
      "          27       0.01      1.00      0.02       246\n",
      "          28       0.00      0.00      0.00       246\n",
      "          29       0.00      0.00      0.00       164\n",
      "          30       0.00      0.00      0.00       164\n",
      "          31       0.00      0.00      0.00       164\n",
      "          32       0.00      0.00      0.00       153\n",
      "          33       0.00      0.00      0.00       166\n",
      "          34       0.00      0.00      0.00       166\n",
      "          35       0.00      0.00      0.00       150\n",
      "          36       0.00      0.00      0.00       154\n",
      "          37       0.00      0.00      0.00       130\n",
      "          38       0.00      0.00      0.00       156\n",
      "          39       0.00      0.00      0.00       166\n",
      "          40       0.00      0.00      0.00       156\n",
      "          41       0.00      0.00      0.00       234\n",
      "          42       0.00      0.00      0.00        99\n",
      "          43       0.00      0.00      0.00       166\n",
      "          44       0.00      0.00      0.00       328\n",
      "          45       0.00      0.00      0.00       164\n",
      "          46       0.00      0.00      0.00       166\n",
      "          47       0.00      0.00      0.00       166\n",
      "          48       0.00      0.00      0.00       164\n",
      "          49       0.00      0.00      0.00       158\n",
      "          50       0.00      0.00      0.00       166\n",
      "          51       0.00      0.00      0.00       164\n",
      "          52       0.00      0.00      0.00       166\n",
      "          53       0.00      0.00      0.00       157\n",
      "          54       0.00      0.00      0.00       166\n",
      "          55       0.00      0.00      0.00       166\n",
      "          56       0.00      0.00      0.00       156\n",
      "          57       0.00      0.00      0.00       157\n",
      "          58       0.00      0.00      0.00       166\n",
      "          59       0.00      0.00      0.00       164\n",
      "          60       0.00      0.00      0.00       166\n",
      "          61       0.00      0.00      0.00       166\n",
      "          62       0.00      0.00      0.00       166\n",
      "          63       0.00      0.00      0.00       166\n",
      "          64       0.00      0.00      0.00       166\n",
      "          65       0.00      0.00      0.00       142\n",
      "          66       0.00      0.00      0.00       102\n",
      "          67       0.00      0.00      0.00       166\n",
      "          68       0.00      0.00      0.00       246\n",
      "          69       0.00      0.00      0.00       164\n",
      "          70       0.00      0.00      0.00       164\n",
      "          71       0.00      0.00      0.00       160\n",
      "          72       0.00      0.00      0.00       218\n",
      "          73       0.00      0.00      0.00       178\n",
      "          74       0.00      0.00      0.00       150\n",
      "          75       0.00      0.00      0.00       155\n",
      "          76       0.00      0.00      0.00       146\n",
      "          77       0.00      0.00      0.00       160\n",
      "          78       0.00      0.00      0.00       164\n",
      "          79       0.00      0.00      0.00       166\n",
      "          80       0.00      0.00      0.00       164\n",
      "          81       0.00      0.00      0.00       246\n",
      "          82       0.00      0.00      0.00       164\n",
      "          83       0.00      0.00      0.00       164\n",
      "          84       0.00      0.00      0.00       232\n",
      "          85       0.00      0.00      0.00       166\n",
      "          86       0.00      0.00      0.00       234\n",
      "          87       0.00      0.00      0.00       102\n",
      "          88       0.00      0.00      0.00       166\n",
      "          89       0.00      0.00      0.00       222\n",
      "          90       0.00      0.00      0.00       237\n",
      "          91       0.00      0.00      0.00       166\n",
      "          92       0.00      0.00      0.00       166\n",
      "          93       0.00      0.00      0.00       148\n",
      "          94       0.00      0.00      0.00       234\n",
      "          95       0.00      0.00      0.00       222\n",
      "          96       0.00      0.00      0.00       222\n",
      "          97       0.00      0.00      0.00       164\n",
      "          98       0.00      0.00      0.00       164\n",
      "          99       0.00      0.00      0.00       166\n",
      "         100       0.00      0.00      0.00       163\n",
      "         101       0.00      0.00      0.00       166\n",
      "         102       0.00      0.00      0.00       151\n",
      "         103       0.00      0.00      0.00       142\n",
      "         104       0.00      0.00      0.00       304\n",
      "         105       0.00      0.00      0.00       164\n",
      "         106       0.00      0.00      0.00       153\n",
      "         107       0.00      0.00      0.00       150\n",
      "         108       0.00      0.00      0.00       151\n",
      "         109       0.00      0.00      0.00       150\n",
      "         110       0.00      0.00      0.00       150\n",
      "         111       0.00      0.00      0.00       166\n",
      "         112       0.00      0.00      0.00       164\n",
      "         113       0.00      0.00      0.00       166\n",
      "         114       0.00      0.00      0.00       164\n",
      "         115       0.00      0.00      0.00       162\n",
      "         116       0.00      0.00      0.00       164\n",
      "         117       0.00      0.00      0.00       246\n",
      "         118       0.00      0.00      0.00       166\n",
      "         119       0.00      0.00      0.00       166\n",
      "         120       0.00      0.00      0.00       246\n",
      "         121       0.00      0.00      0.00       225\n",
      "         122       0.00      0.00      0.00       246\n",
      "         123       0.00      0.00      0.00       160\n",
      "         124       0.00      0.00      0.00       164\n",
      "         125       0.00      0.00      0.00       228\n",
      "         126       0.00      0.00      0.00       127\n",
      "         127       0.00      0.00      0.00       153\n",
      "         128       0.00      0.00      0.00       158\n",
      "         129       0.00      0.00      0.00       249\n",
      "         130       0.00      0.00      0.00       157\n",
      "\n",
      "    accuracy                           0.01     22688\n",
      "   macro avg       0.00      0.01      0.00     22688\n",
      "weighted avg       0.00      0.01      0.00     22688\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_true = test_data_gen.classes\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
