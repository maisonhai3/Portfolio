{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-66da9fa3b396>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# Ploting libs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moffline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0miplot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_subplots\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_objects\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mgo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "'''General Libraries'''\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "'''Statistic'''\n",
    "import scipy\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "import datetime\n",
    "\n",
    "'''Scikit Learn'''\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split,cross_val_score, GridSearchCV\n",
    "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
    "\n",
    "'''Ploting Libraries'''\n",
    "from plotly.offline import iplot, plot\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\" \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_palette('RdBu')\n",
    "\n",
    "'''Miscellaneous'''\n",
    "from yellowbrick.regressor import ResidualsPlot\n",
    "import missingno as msno\n",
    "\n",
    "'''Seeds'''\n",
    "import random\n",
    "random.seed(10)\n",
    "np.random.seed(11)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('results.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A very first glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observations                 :  31984\n",
      "Features                     :  21\n"
     ]
    }
   ],
   "source": [
    "print('Observations                 : ', df.shape[0])\n",
    "print('Features                     : ', df.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10k           object\n",
       "20k           object\n",
       "25k           object\n",
       "30k           object\n",
       "35k           object\n",
       "40k           object\n",
       "5k            object\n",
       "age            int64\n",
       "bib           object\n",
       "city          object\n",
       "country       object\n",
       "ctz           object\n",
       "division       int64\n",
       "gender        object\n",
       "genderdiv      int64\n",
       "half          object\n",
       "name          object\n",
       "official     float64\n",
       "overall        int64\n",
       "pace         float64\n",
       "state         object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_index(axis=1).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10k</th>\n",
       "      <th>name</th>\n",
       "      <th>division</th>\n",
       "      <th>25k</th>\n",
       "      <th>gender</th>\n",
       "      <th>age</th>\n",
       "      <th>official</th>\n",
       "      <th>bib</th>\n",
       "      <th>genderdiv</th>\n",
       "      <th>ctz</th>\n",
       "      <th>35k</th>\n",
       "      <th>overall</th>\n",
       "      <th>pace</th>\n",
       "      <th>state</th>\n",
       "      <th>30k</th>\n",
       "      <th>5k</th>\n",
       "      <th>half</th>\n",
       "      <th>20k</th>\n",
       "      <th>country</th>\n",
       "      <th>city</th>\n",
       "      <th>40k</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.37</td>\n",
       "      <td>Yamamoto, Hiroyuki</td>\n",
       "      <td>8</td>\n",
       "      <td>47.67</td>\n",
       "      <td>M</td>\n",
       "      <td>47</td>\n",
       "      <td>85.25</td>\n",
       "      <td>W1</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71.40</td>\n",
       "      <td>8</td>\n",
       "      <td>3.27</td>\n",
       "      <td>NaN</td>\n",
       "      <td>59.18</td>\n",
       "      <td>8.02</td>\n",
       "      <td>39.72</td>\n",
       "      <td>37.65</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Fukuoka</td>\n",
       "      <td>80.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>32.58</td>\n",
       "      <td>Jeptoo, Rita</td>\n",
       "      <td>1</td>\n",
       "      <td>82.43</td>\n",
       "      <td>F</td>\n",
       "      <td>33</td>\n",
       "      <td>138.95</td>\n",
       "      <td>F1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.37</td>\n",
       "      <td>21</td>\n",
       "      <td>5.30</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.33</td>\n",
       "      <td>16.22</td>\n",
       "      <td>69.47</td>\n",
       "      <td>65.83</td>\n",
       "      <td>KEN</td>\n",
       "      <td>Eldoret</td>\n",
       "      <td>132.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16.62</td>\n",
       "      <td>Van Dyk, Ernst F.</td>\n",
       "      <td>1</td>\n",
       "      <td>45.80</td>\n",
       "      <td>M</td>\n",
       "      <td>41</td>\n",
       "      <td>80.60</td>\n",
       "      <td>W2</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.42</td>\n",
       "      <td>1</td>\n",
       "      <td>3.08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>56.45</td>\n",
       "      <td>7.75</td>\n",
       "      <td>38.03</td>\n",
       "      <td>36.10</td>\n",
       "      <td>RSA</td>\n",
       "      <td>Paarl</td>\n",
       "      <td>76.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>32.57</td>\n",
       "      <td>Dibaba, Mare</td>\n",
       "      <td>3</td>\n",
       "      <td>82.43</td>\n",
       "      <td>F</td>\n",
       "      <td>24</td>\n",
       "      <td>140.58</td>\n",
       "      <td>F2</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.37</td>\n",
       "      <td>27</td>\n",
       "      <td>5.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.33</td>\n",
       "      <td>16.20</td>\n",
       "      <td>69.47</td>\n",
       "      <td>65.83</td>\n",
       "      <td>ETH</td>\n",
       "      <td>Shoa</td>\n",
       "      <td>132.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.12</td>\n",
       "      <td>Hokinoue, Kota</td>\n",
       "      <td>2</td>\n",
       "      <td>46.37</td>\n",
       "      <td>M</td>\n",
       "      <td>40</td>\n",
       "      <td>81.23</td>\n",
       "      <td>W3</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>67.83</td>\n",
       "      <td>2</td>\n",
       "      <td>3.10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>57.03</td>\n",
       "      <td>8.02</td>\n",
       "      <td>38.60</td>\n",
       "      <td>36.58</td>\n",
       "      <td>JPN</td>\n",
       "      <td>Nogata Fukuoka</td>\n",
       "      <td>76.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>32.58</td>\n",
       "      <td>Sumgong, Jemima Jelagat</td>\n",
       "      <td>4</td>\n",
       "      <td>82.45</td>\n",
       "      <td>F</td>\n",
       "      <td>29</td>\n",
       "      <td>140.68</td>\n",
       "      <td>F3</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>116.37</td>\n",
       "      <td>28</td>\n",
       "      <td>5.37</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.33</td>\n",
       "      <td>16.22</td>\n",
       "      <td>69.47</td>\n",
       "      <td>65.83</td>\n",
       "      <td>KEN</td>\n",
       "      <td>Nandi</td>\n",
       "      <td>132.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>17.65</td>\n",
       "      <td>Hug, Marcel E.</td>\n",
       "      <td>4</td>\n",
       "      <td>47.67</td>\n",
       "      <td>M</td>\n",
       "      <td>28</td>\n",
       "      <td>84.65</td>\n",
       "      <td>W4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.23</td>\n",
       "      <td>4</td>\n",
       "      <td>3.23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>58.60</td>\n",
       "      <td>8.38</td>\n",
       "      <td>39.72</td>\n",
       "      <td>37.65</td>\n",
       "      <td>SUI</td>\n",
       "      <td>Neuenkirch</td>\n",
       "      <td>79.83</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     10k                     name  division    25k gender  age  official bib  \\\n",
       "0  17.37       Yamamoto, Hiroyuki         8  47.67      M   47     85.25  W1   \n",
       "1  32.58             Jeptoo, Rita         1  82.43      F   33    138.95  F1   \n",
       "2  16.62        Van Dyk, Ernst F.         1  45.80      M   41     80.60  W2   \n",
       "3  32.57             Dibaba, Mare         3  82.43      F   24    140.58  F2   \n",
       "4  17.12           Hokinoue, Kota         2  46.37      M   40     81.23  W3   \n",
       "5  32.58  Sumgong, Jemima Jelagat         4  82.45      F   29    140.68  F3   \n",
       "6  17.65           Hug, Marcel E.         4  47.67      M   28     84.65  W4   \n",
       "\n",
       "   genderdiv  ctz     35k  overall  pace state    30k     5k   half    20k  \\\n",
       "0          8  NaN   71.40        8  3.27   NaN  59.18   8.02  39.72  37.65   \n",
       "1          1  NaN  116.37       21  5.30   NaN  99.33  16.22  69.47  65.83   \n",
       "2          1  NaN   67.42        1  3.08   NaN  56.45   7.75  38.03  36.10   \n",
       "3          3  NaN  116.37       27  5.37   NaN  99.33  16.20  69.47  65.83   \n",
       "4          2  NaN   67.83        2  3.10   NaN  57.03   8.02  38.60  36.58   \n",
       "5          4  NaN  116.37       28  5.37   NaN  99.33  16.22  69.47  65.83   \n",
       "6          4  NaN   70.23        4  3.23   NaN  58.60   8.38  39.72  37.65   \n",
       "\n",
       "  country            city     40k  \n",
       "0     JPN         Fukuoka   80.43  \n",
       "1     KEN         Eldoret  132.10  \n",
       "2     RSA           Paarl   76.10  \n",
       "3     ETH            Shoa  132.95  \n",
       "4     JPN  Nogata Fukuoka   76.72  \n",
       "5     KEN           Nandi  132.95  \n",
       "6     SUI      Neuenkirch   79.83  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "* ***Landsize and BuildingArea*** \n",
    "* ***Date*** \n",
    "* ***Suburb, Address, SellerG***: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Missing Data: A Quick Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Percent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ctz</th>\n",
       "      <td>30740</td>\n",
       "      <td>0.961106</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>state</th>\n",
       "      <td>2576</td>\n",
       "      <td>0.080540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>city</th>\n",
       "      <td>1</td>\n",
       "      <td>0.000031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40k</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>genderdiv</th>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Numbers   Percent\n",
       "ctz          30740  0.961106\n",
       "state         2576  0.080540\n",
       "city             1  0.000031\n",
       "40k              0  0.000000\n",
       "genderdiv        0  0.000000"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A Brief of Missing data\n",
    "\n",
    "total_miss   = df.isnull().sum().sort_values(ascending=False)\n",
    "percent      = total_miss / df.shape[0]\n",
    "\n",
    "table = pd.concat([total_miss, percent], axis=1, keys=['Numbers', 'Percent'])\n",
    "table.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10k          0\n",
       "name         0\n",
       "division     0\n",
       "25k          0\n",
       "gender       0\n",
       "age          0\n",
       "official     0\n",
       "bib          0\n",
       "genderdiv    0\n",
       "ctz          0\n",
       "35k          0\n",
       "overall      0\n",
       "pace         0\n",
       "state        0\n",
       "30k          0\n",
       "5k           0\n",
       "half         0\n",
       "20k          0\n",
       "country      0\n",
       "city         0\n",
       "40k          0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This case: zero means missing. Check for it.\n",
    "(df==0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detection by IQR Rule\n",
    "\n",
    "***IQR Rule***  \n",
    "\n",
    "This is a renowned technique to detecting outliers. To apply this rule, first we need to define several stuffs.\n",
    "\n",
    "***Q1***: the quantile at 25%.\n",
    "\n",
    "***Q3***: the quantile at 75%.\n",
    "\n",
    "***IQR*** = Q3 - Q1.\n",
    "\n",
    "Then, any value stands out of range **[Q1 - 1.5 IQR, Q3 + 1.5 IQR]** would be considered an outlier.\n",
    "\n",
    "***Note:*** The IQR rule would be praticed on numerical features only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR score\n",
    "def IQR_outlier_detect(data=df, features=[]):\n",
    "    for feature in features:\n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outside_IQR = (data[feature]<=(Q1-1.5*IQR)) | ((Q3+1.5*IQR)<=data[feature])  \n",
    "        outside_IQR = outside_IQR.sum()        \n",
    "        \n",
    "        print('Outside of IQR: %s -- Total: %d -- percent %2.2f'% (feature, outside_IQR, outside_IQR/df.shape[0]))\n",
    "    return\n",
    "\n",
    "IQR_outlier_detect(df, features=['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, detect Outliers\n",
    "features = continuous_features + discrete_features\n",
    "IQR_outlier_detect(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Outliers\n",
    "def IQR_outlier_remove(data=df, features=[]):\n",
    "    for feature in features:\n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        # the core: the ~ is a must to avoid removing NaN.\n",
    "        outside_IQR = (data[feature]<=(Q1-1.5*IQR)) | ((Q3+1.5*IQR)<=data[feature])\n",
    "        data = data[~outside_IQR]\n",
    "        print('Cleaning: ', feature)\n",
    "        print('Q1: %2.2f', Q1)\n",
    "        print('Q2: %2.2f', Q3)\n",
    "        print('After cleaning, data left: %d \\n' % (data.shape[0]))\n",
    "        \n",
    "    return data\n",
    "\n",
    "# Driving code\n",
    "features = continuous_features + discrete_features\n",
    "df = IQR_outlier_remove(df, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How much observations left?\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Descriptive Statistic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(percentiles=[0.01, 0.25, 0.75, 0.99])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Numerics\n",
    "***Datatype***\n",
    " * ***Postcode and Propertycount:*** first, they should have been categorical by nature, but being numerical. Second, Postcode has 211 uniques and Propertycount has 342, so even with converts into categorical, one-hot-encode will be useless. I will remove them.\n",
    " \n",
    "***Abnormality***\n",
    " * ***Some palaces on sale*** with: 30 Bedroom 2, 26 slots of Car parking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texts with too many of uniques\n",
    "df.drop(['Postcode', 'Propertycount'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.describe(include='O').sort_values(axis=1, by=['unique'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Categories\n",
    "* ***CouncilArea*** has 33 of unique values, though still are able to apply one-hot-encode, but it will burden Linear Regression performance. It would be removed.\n",
    "* ***Regionname, Type, Method*** has pretty small number of distinct values. They are deserved to one-hot-encode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('CouncilArea', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-do latter\n",
    "* ***Regionname, Type, Method***: one-hot-encode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Classify features based on Datatypes, helpful for EDA.\n",
    "\n",
    "continuous_features = ['Price',      'Distance']\n",
    "\n",
    "discrete_features  = ['Bathroom',    'Bedroom2',       'Car',        'Rooms']\n",
    "\n",
    "category_features  = ['Type',        'Method',         'Regionname']\n",
    "                     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.1 The Target: Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(df['Price'], fit=norm);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price is ***skewed***, but be able to nomalized by removing extreme high points on the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.2 Univariate analyze: Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[continuous_features].hist(bins=40, figsize=(18,9))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Distance*** are skewed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[discrete_features].hist(bins=40, figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on Discrete Features\n",
    "***Potential Outliers***\n",
    "Most of observations have:\n",
    "* Bathroom < 5,\n",
    "* Bedroom2 < 10,\n",
    "* Car      < 10,\n",
    "* Rooms    < 6.  \n",
    "\n",
    "So, points standing out of these boundaries probaly are outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.3 Bivariate analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First try for Total sales per Region\n",
    "\n",
    "# plotly.offline.init_notebook_mode(connected=True)\n",
    "\n",
    "regions = df.Regionname.unique()\n",
    "total_values_per_region = [df['Price'][df.Regionname==region].sum() for region in regions]\n",
    "\n",
    "fig = px.bar(y=regions, x=total_values_per_region,\n",
    "             title='Total Sales per Regions', orientation='h',\n",
    "             template='plotly_white')\n",
    "\n",
    "fig.update_layout(xaxis={'title':'Price'},\n",
    "                  yaxis={'title':'Regions'})\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Regions*** somehow play an important role in Sales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.box(df, x='Regionname', y='Price', template='simple_white')\n",
    "fig.update_layout(title='Price by Regions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Strange 'Outliers'***   \n",
    "A considerable number of Price are far from their quartiles, I afraid that Z-score, 3-sigma, or IQR - detecting outlier strategies will remove a lot of data, lets see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IQR score\n",
    "def IQR_outlier_detect(data=df, features=[]):\n",
    "    for feature in features:\n",
    "        Q1 = data[feature].quantile(0.25)\n",
    "        Q3 = data[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        outside_IQR = (data[feature]<=(Q1-1.5*IQR)) | ((Q3+1.5*IQR)<=data[feature])  \n",
    "        outside_IQR = outside_IQR.sum()        \n",
    "        \n",
    "        print('Outside of IQR: %s -- Total: %d -- percent %2.2f'% (feature, outside_IQR, outside_IQR/df.shape[0]))\n",
    "    return\n",
    "\n",
    "IQR_outlier_detect(df, features=['Price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***No problem, Price is fine***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(df, x='Longtitude', y='Lattitude', color='Price')\n",
    "fig.update_layout(title='Price by Locations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sale houses tend to locate in the map central.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.4 Multivariate analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price vs Continuous Features\n",
    "\n",
    "corr_matrix = df[continuous_features].corr()\n",
    "\n",
    "figure = plt.figure(figsize=(16,12))\n",
    "\n",
    "mask = np.triu(corr_matrix) # Hide the upper part.\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidths=0.5, cmap=\"YlGnBu\", mask=mask)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing seems to be meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price vs Discrete Features\n",
    "\n",
    "corr_matrix = df[discrete_features + ['Price']].corr()\n",
    "\n",
    "figure = plt.figure(figsize=(16,12))\n",
    "\n",
    "mask = np.triu(corr_matrix) # Hide the upper part.\n",
    "sns.heatmap(corr_matrix, annot=True, fmt='.2f', linewidths=0.5, cmap=\"YlGnBu\", mask=mask)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Rooms and Bedroom2*** is a twins, so keep Rooms and drop the latter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop('Bedroom2', axis=1, inplace=True)\n",
    "\n",
    "discrete_features.remove('Bedroom2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_scaler = ['Rooms', 'Distance', 'Bathroom', 'Car',\n",
    "                        'Lattitude', 'Longtitude',\n",
    "                        'Month', 'Year']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "for feature in features_to_scaler:\n",
    "    df_std[feature] = scaler.fit_transform(df_std[feature].values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. One Hot Encode\n",
    "With intending to knn imputing on missing data, but knn only works with numerical, not categorical, so the encoding is performed up front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_std.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode = pd.get_dummies(df_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies for  Missing Data\n",
    "<a href=\"https://ibb.co/fXC5QMG\"><img src=\"https://i.ibb.co/TwHSrcq/Missing-Data.png\" alt=\"Missing-Data\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumption: all MCAR.\n",
    "\n",
    "Selection of methods must base on the nature of missing data, whether they are MCAR, MAR, or MNAR. I know a research on those are essential, but in this entry-level assignment, I will skip it to focus on the major section Modelling.\n",
    "\n",
    "Therefore, let assumpt all columns are MCAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A Brief of Missing data\n",
    "\n",
    "total_miss   = df.isnull().sum().sort_values(ascending=False)\n",
    "\n",
    "percent      = total_miss / df.shape[0]\n",
    "\n",
    "table = pd.concat([total_miss, percent], axis=1, keys=['Numbers', 'Percent'])\n",
    "print(table.head(8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Strategies on Choices\n",
    "\n",
    " * ***Hand-in-hand pattern***: If a row lacks Car value, moreoften lacks values in Bathroom, Longtitude, Lattitude, and vice versa. Please scroll a half page down then look at the graph, you'll see that most of cells are around 1, which means our missing data are very centralized in specific rows.\n",
    "      \n",
    " \n",
    " * ***K-nn Imputation is by far the best***. Reason is the way real estate market working: houses with similar specifications, close-by location, usually sold in the same price level. So, k-nn is a nice choice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.heatmap(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simply Put\n",
    "***Listwise deleting***: Region Name and Distance.\n",
    "\n",
    "***K-nn approach***: all the rest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encode.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-nn imputation\n",
    "neighbors = 10\n",
    "imputer = KNNImputer(n_neighbors=neighbors)\n",
    "\n",
    "df_filled = imputer.fit_transform(df_encode)\n",
    "\n",
    "# to Dataframe\n",
    "df_filled = pd.DataFrame(df_filled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Assign to X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_filled[1]\n",
    "\n",
    "X = df_filled.drop(labels=1, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assumptions of Linear Regression\n",
    "\n",
    "Beforehand modeling or tuning, firstly we need to acknowledge of Assumptions of Linear Regression.\n",
    "\n",
    "* ***Normality of X and y***. Or by a more specific term: multivariate normality. Hum, dangerous-look words..\n",
    "* ***Linearity of X and y***. Capital X means a plural of features, columns.\n",
    "* ***Homoscedasticity***. Namely, variance of residuals are constant. There are several others explanation of homoscedasticity, but this one is nice and simple at most, especially for Residual plots.\n",
    "\n",
    "\n",
    "It seems like a lot of works, but fortunely could be done just by Residual plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Normality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normality of y\n",
    "sns.distplot(y, fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(y, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Price is not very normal. It shows peakedness, skewness and does not follow the diagonal line.\n",
    "\n",
    "Let's transform it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.log(y)\n",
    "\n",
    "# Check again\n",
    "# Normality of y\n",
    "sns.distplot(y, fit=norm);\n",
    "fig = plt.figure()\n",
    "res = stats.probplot(y, plot=plt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normality of X\n",
    "\n",
    "Take a look at X then we'll see lots of negative values, which are incompetence for log transformations. Of course, we can still perform np.log() for X, but it will return NaN for negative values, and damages our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Searching for Linearity and Homoscedasticity\n",
    "\n",
    "I will leave them blank because I don't know how to do it for now. Sorry, it must be a gap in my knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling\n",
    "\n",
    "The most important part of this sections is ***B - Linear Regression with Cross Validation***, that I am carrying on both modeling and tuning carefully. The reasons:\n",
    "* Linear regression with Holdout, aka 1-fold cross validation, are highly dependent on luck that I dislike, so it is removal.\n",
    "* Linear regression with PCA is just a CV linear regression with additional steps. It's better to detail the CV linear regression then assuming the PCA one are similiar.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train and y_train for models to learn with folding by the cross validation, where X_test, y_test would be untouch till the final scoring. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A - Linear Regression with Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = Ridge(alpha=0)\n",
    "\n",
    "A.fit(X_train, y_train)\n",
    "print(\"A's score: %2.4f\" % A.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, a nice number.Then, 0.6959 would be our baseline for further tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B - Linear Regression with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# B is the same as A but with CV\n",
    "\n",
    "B = RidgeCV(alphas=[0], cv=5, scoring='r2')\n",
    "\n",
    "B.fit(X_train, y_train)\n",
    "print(\"A's score: %2.4f\" % B.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the best k-folds\n",
    "\n",
    "B_score = []\n",
    "cv = []\n",
    "\n",
    "for i in range(2, 11):\n",
    "    model = Ridge(alpha=0, normalize=True)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=i).mean()\n",
    "    if score<0 : score = 0\n",
    "    B_score.append(round(score, 5))\n",
    "    cv.append(i)\n",
    "    \n",
    "    print(\"cv: %d --- score: %2.5f\" % (i, score))\n",
    "    \n",
    "B_score = [0 if score<0 else score for score in B_score]\n",
    "print(B_score)\n",
    "\n",
    "px.line(x=cv, y=B_score, \n",
    "        template='simple_white', \n",
    "        title='<b>K-fold vs R2</b>',\n",
    "        labels={'x':'K-fold', 'y':'R2'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** 8 K-fold is the best***, though not the by far best. Let fix the k-fold down. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning B - Linear Regression with cv=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 2 parameters to tune:\n",
    "* How strong the regularization.\n",
    "* Should we normalize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'alpha':[100, 30, 21, 20, 19.5, 19, 18.5, 18, 17, 17.5, 16, 15, 14, 13.5, 13, 12.5, 12, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.7, 7.6, 7.5, 7.4, 7.3, 7, 6, 5, 4.5, 4, 3.5, 3, 1, 0.3, 0.1, 0.03, 0.01, 0],\n",
    "          'normalize': (True, False)}\n",
    "\n",
    "model = Ridge()\n",
    "gsc = GridSearchCV(estimator=model, param_grid=params, n_jobs=-1)\n",
    "gsc.fit(X_train, y_train)\n",
    "\n",
    "best = gsc.best_params_\n",
    "score = gsc.score(X_test, y_test)\n",
    "print('With : ', best)\n",
    "print('Score: %2.4f' % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best choice: alpha 7.6 and normalize False. \n",
    "\n",
    "Yes, normalize must be False - turned off, cause we perform a standard scaling already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With those best params, plot: Residuals vs Prediction\n",
    "\n",
    "B = gsc.best_estimator_\n",
    "B.fit(X_train, y_train)\n",
    "print(\"B's score: %2.4f\" % B.score(X_test, y_test))\n",
    "\n",
    "visualizer = ResidualsPlot(B)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comments\n",
    "* Homoscedastic. Yes, we got it. The shape is not fan-out, not spreading. Points locate within a quite parallel limits.\n",
    "* Normality. We got it, too. On the scatter plot, there are a bit outliers on the upper, but no any dense on one side. Then, look at the histogram on the right, quite perfect nomarl, huh.\n",
    "* Linearity between X and y. I am not so sure. It is worth an extensive study.\n",
    "\n",
    "* Outliers. There are some of them on the higher top. I afraid that somehow these outliers sneaked into data after all the scaling and normalizing to destroy our normality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Comments for the OLDER version of B***\n",
    "\n",
    "> For my presentation at class.\n",
    "\n",
    "In this graph of Residuals against Predicted values, the ***distribution*** is:\n",
    "1. In fan-out shape: an identify of not constant variance of residuals, or namely ***Heteroscedasticity***.\n",
    "2. A little curve or bend: probably is a proof of ***non-linear***.\n",
    "\n",
    "So we got two violations here: ***non-homoscedasticity*** and ***non-linearity*** of X and y. Mention that both problems lay in natural of data, not in the linear model. Nothing in hell we can do with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import PredictionError\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "model = PredictionError(B)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)\n",
    "model.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C - LR with PCA and Holdout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_train)\n",
    "\n",
    "cumsum = pca.explained_variance_ratio_.cumsum() // 0.01\n",
    "n_comp = [i for i in range(1, len(cumsum)+1, 1)]\n",
    "\n",
    "print(cumsum)\n",
    "px.bar(y=cumsum, x=n_comp, text=cumsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to those numbers, with 10 principal components, we can loss no more than 10% information. Let's choose ***n_components=10***."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "                ('PCA', PCA(n_components=10)),\n",
    "                ('Linear Regression', Ridge(alpha=0, normalize=True))])\n",
    "pipe.fit(X_train, y_train)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh, dimensionality reduction means lossing in information as well as lossing in our R2 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D - LR with PCA and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# D\n",
    "step = [( 'PCA'     , PCA()   ),\n",
    "        ( 'Lin_Reg' , RidgeCV(alphas=[0], cv=7) )]\n",
    "\n",
    "D = Pipeline(step)\n",
    "D.fit(X_train, y_train)\n",
    "score = D.score(X_test, y_test)\n",
    "print(\"D's score: %2.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 parameters to tune:\n",
    "* Number of Principle components in PCA,\n",
    "* How strong the regularization.\n",
    "* Should we normalize the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step = [( 'PCA'     , PCA()   ),\n",
    "        ( 'Lin_Reg' , Ridge() )]\n",
    "pipe = Pipeline(step)\n",
    "\n",
    "params = {'PCA__n_components' : range(1,24),\n",
    "          'Lin_Reg__alpha'    : [100, 30, 21, 20, 19.5, 19, 18.5, 18, 17, 17.5, 16, 15, 14, 13.5, 13, 12.5, 12, 11, 10.5, 10, 9.5, 9, 8.5, 8, 7.7, 7.6, 7.5, 7.4, 7.3, 7, 6, 5, 4.5, 4, 3.5, 3, 1, 0.3, 0.1, 0.03, 0.01, 0],\n",
    "          'Lin_Reg__normalize': [True, False]}\n",
    "\n",
    "gsc = GridSearchCV(pipe, param_grid=params, cv=7)\n",
    "gsc.fit(X_train, y_train)\n",
    "\n",
    "best = gsc.best_params_\n",
    "score = gsc.score(X_test, y_test)\n",
    "print('With : ', best)\n",
    "print('Score: %2.4f' % score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = gsc.best_estimator_\n",
    "D.fit(X_train, y_train)\n",
    "print(\"B's score: %2.4f\" % D.score(X_test, y_test))\n",
    "\n",
    "visualizer = ResidualsPlot(D)\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With tons of tuning, the model advance only 0.0001 R2 score. It is not worth the effort."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 11. Conclusion.\n",
    "\n",
    "1. Cleaning data is very challenging. It took me dozens of hours and bunchs of effor to get data in shape. I doubt when people say most of time in data science, you will deal with data cleaning. Now I sadly know it's true.\n",
    "2. In searching for Normality of X, I was unable to log transform X because of negative values. These negatives came from standard scaling performed beforehand. I am thinking that if I first perform log-transformation, then scale the data later, so I could get benefits from both process with nicer distributions of X features.\n",
    "3. Tuning did not make sense as much as I hoped. The baseline of very simple linear regression was 0.69 in R2. Then I did not get any improvement after all the tuning. There are some reasonable explains:\n",
    "    * I perfectly cleaned the data. Oh, I hope so.\n",
    "    * Ridge - a linear regression with L2 regularization was too simple for predictions.\n",
    "    * Is there a better transform than a log, like a square root?"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "178.391px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 360,
   "position": {
    "height": "40px",
    "left": "645px",
    "right": "20px",
    "top": "152px",
    "width": "388px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
